# 3.5.2. 预期奖励

> 最大化即时预期回报会导致贪婪的计划。

由于动作的效果是不确定的，因此机器人无法确定性地预测特定动作$$a$$的下一个状态$$x^{\prime}$$的值 ;因此，无法评估$$R(x,a,x^{\prime})$$。在这种情况下，我们可以使用预期奖励来评估作的随机效应。正如我们在上一章中所看到的，如果从状态$$x$$执行操作$$a$$，这种期望将提供对将获得的平均奖励的良好估计，即使它没有保证在任何特定时刻将获得的奖励。

Wede 表示在 state $$x$$中执行操作$$a$$的预期奖励$$\overline{R}(x,a)$$\
，其值是通过评估期望值获得的

$$
\begin{equation}
\bar{R}(x,a) \doteq E[R(x,a,X')] = \sum_{x'} P(x'|x,a) R(x, a, x').
\end{equation}
$$

请注意，我们使用大写$$X^{\prime}$$字母来表示期望是针对随机的 next 状态。因此，总和是所有可能的下一个状态，并且每个下一个状态的奖励$$R(x,a,x^{\prime})$$由通过执行 action $$a$$从状态$$x$$到达该状态的概率$$P(x^{\prime}|x,a)$$加权。

为了计算代码中的预期奖励，我们首先创建一个多维数组$$R(x,a,x^{\prime})$$，其中包含每个可能的转换$$x,a \rightarrow x^{\prime}$$的奖励：

```python
R = np.empty((5, 4, 5), float)
for x, room_x in enumerate(vacuum.rooms):
    for a, action in enumerate(vacuum.action_space):
        for y, room_y in enumerate(vacuum.rooms):
            R[x, a, y] = 10.0 if room_y == "Living Room" else 0.0

# For example, taking action "L" in "Kitchen":
R[vacuum.rooms.index("Kitchen"), vacuum.action_space.index("L")]
```

```python
array([10.,  0.,  0.,  0.,  0.])
```

上面的结果很容易解释：如果机器人从厨房开始并向左移动，则到达客厅（索引 0）时，它会获得 10 的奖励，否则为 0。

观察$$R$$是一个多维数组，形状为$$(5,4,5)$$ ，对应于 reward 函数$$R(x,a,x^{\prime})$$的参数的维数。我们可以对转换概率做同样的事情，创建一个多维数组$$T$$，其中包含每个可能的转换$$x \rightarrow a \rightarrow x^{\prime}$$的 probability $$P(x^{\prime}|a,x)$$条目：

```python
# First create a conditional with ids 0, 1, and 2 for state, action, and next state:
conditional = gtsam.DiscreteConditional((2,5), [(0,5), (1,4)], vacuum.action_spec)

# Now use `enumerate` to iterate over all possible assignments to the conditional:
T = np.empty((5, 4, 5), float)
for assignment, P_y_given_ax in conditional.enumerate():
    x, a, y = assignment[0], assignment[1], assignment[2]
    T[x, a, y] = P_y_given_ax

# For example, taking action "L" in "Kitchen":
T[vacuum.rooms.index("Kitchen"), vacuum.action_space.index("L")]
```

```python
array([0.8, 0.2, 0. , 0. , 0. ])
```

正如预期的那样，如果我们在机器人在厨房时执行动作 L，我们最终有 80% 的几率进入客厅，而有 20% 的几率留在原地（厨房）。

然后，最后，计算特定 state-action 对$$(x,a)$$的预期奖励$$\overline{R}(x,a)$$成为一个简单的点积：

```python
x, a = vacuum.rooms.index("Kitchen"), vacuum.action_space.index("L")
print(f"Expected reward (Kitchen, L) = {np.dot(T[x,a], R[x,a])}")
```

```python
Expected reward (Kitchen, L) = 8.0
```

这是完全有道理的：如果我们留在厨房，我们得到的奖励为零，但如果我们成功搬到客厅，我们会收到 10 的奖励。不过，这种情况只发生在 80% 的情况下，因此预期奖励仅为 8。

有了预期奖励的定义，我们可以引入第一个贪婪计划算法。给定当前状态$$x_k$$，执行使预期奖励最大化的作：

$$
\begin{equation}
a_k^* = \arg  \max_{a \in {\cal A}} E[ R(x_k, a, X_{k+t})]
\end{equation}
$$

例如，在厨房中，我们可以计算所有作的预期奖励：

```python
x = vacuum.rooms.index("Kitchen")
for a in range(4):
    print(f"Expected reward ({vacuum.rooms[x]}, {vacuum.action_space[a]}) = {np.dot(T[x,a], R[x,a])}")
```

```python
Expected reward (Kitchen, L) = 8.0
Expected reward (Kitchen, R) = 0.0
Expected reward (Kitchen, U) = 0.0
Expected reward (Kitchen, D) = 0.0
```

事实证明，$$8.0$$这是我们通过贪婪地选择单个作所能做的最好的事情，因为任何其他作的预期即时奖励为零。因此，每当我们发现自己在厨房里时，一个好的做法是始终向左走！

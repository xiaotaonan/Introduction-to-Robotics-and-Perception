# 3.5.1. 奖励函数

> 奖励函数提供了一种定量方法来评估作的效果。

我们的机器人应该如何评估动作的效果？一种简单的方法可能是指定机器人的目标，然后确定一个动作相对于该目标的有效性，例如，通过使用各种动作的条件概率表。例如，如果目标是到达客厅，我们可以评估特定动作实现目标的概率。

此方法有几个限制。首先，如果机器人当前不在与客厅相邻的房间里，则不清楚如何衡量特定动作可能进入客厅的进度。例如，如果机器人在餐厅，那么移动到厨房还是作为通往客厅的中间台阶移动到走廊会更好？其次，这种方法不允许机器人考虑访问其他州可能带来的好处。例如，即使目标是客厅，但到达厨房可能并不是一个糟糕的结果，例如，如果厨房地板也需要清洁。最后，执行作的好处可能不仅取决于目标，还取决于执行作的状态。例如，从走廊进入客厅可能不如从厨房进入（例如，如果客人可能会到达入口走廊）。

奖励函数提供了一个非常通用的解决方案来解决所有这些限制。让我们$$\mathcal{X}$$表示状态集和$$\mathcal{A}$$作集。reward 函数$$\mathcal{R}:\mathcal{X} \times \mathcal{A} \times \mathcal{X}  \rightarrow R$$，将数字 reward 分配给特定作下的特定状态转换。特别是，$$R(x_k,a_k,x_{k+1})$$是由于执行 action$$a_k$$而从 state $$x_k$$到达 state $$x_{k+1}$$所获得的奖励。

在下文中，我们假设奖励函数是时不变的，即从走廊向上移动到客厅的好处不会随着时间的推移而改变。这允许我们使用更紧凑的表示法$$R(x,a,x^{\prime})$$来表示通过在 state $$x$$中执行 action $$a$$到达（下一个）状态$$x^{\prime}$$而获得的奖励。我们将在本节的其余部分经常使用此表示法。

这种形式的 reward 函数非常通用，允许我们对依赖于上下文的 rewards 进行编码，这些 rewards 取决于执行 action 的状态。将奖励仅指定为状态的函数也很常见。在这种情况下， wederepresent $$R(x)$$by the reward for reaching to state $$x$$，无论应用了什么作，也无论之前的状态如何。下面给出了这种奖励函数的一个示例，它作为 python 函数实现。它只是在进入客厅时返回 10 的奖励：

```python
def reward_function(state:int, action:int, next_state:int):
    """Reward that returns 10 upon entering the living room."""
    return 10.0 if next_state == "Living Room" else 0.0

print(reward_function("Kitchen", "L", "Living Room"))
print(reward_function("Kitchen", "L", "Kitchen"))
```

```
10.0
0.0
```

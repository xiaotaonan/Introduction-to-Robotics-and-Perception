# 3.5.9. 小结

马尔可夫决策过程或 MDP 可用于在随机环境中对决策进行建模，尽管对状态有完整的了解。这是一个内容丰富的主题，我们在本节中引入了许多新概念：

* 奖励函数  $$\mathcal{R}：\mathcal{X} \times \mathcal{A} \times  \mathcal{X} \rightarrow \mathcal{R}$$.
* 在 state $$x$$  中执行 action $$a$$  的预期奖励$$\overline{R}(x,a)$$  ，以及相应的贪婪规划算法。
* 效用函数$$U：\mathcal{A}^n \times \mathcal{X}^{n+1} \rightarrow R$$  ，作为折扣奖励的总和。
* 转出的概念，用于近似作的预期效用。
* 策略$$\pi：\mathcal{X}  \rightarrow \mathcal{A}$$  ，作为从状态到作的映射。
* 使用策略转出来近似 value function$$\mathcal{V}^{\pi}$$  。
* 固定策略的 value 函数的精确计算。

我们将在下一节中介绍最优策略的概念以及如何计算它。MDP 的另外两个扩展我们没有涵盖：

* 当我们无法直接观察状态时，部分可观察的 MDP（或 POMDPS）是合适的。
* 强化学习是一种从数据中学习 MDP 策略的方法。这也将在下一节中介绍。

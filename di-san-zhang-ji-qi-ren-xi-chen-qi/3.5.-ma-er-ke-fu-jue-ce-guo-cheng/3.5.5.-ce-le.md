# 3.5.5. 策略

> 策略是指定在每种状态中要执行的作的函数。

在上面的示例中，动作序列 R，U，L，L 的控制磁带转展栏多次失败。失败的原因似乎很明显：无论状态轨迹如何，robot 都执行了相同的动作序列。如果机器人能够根据当前状态选择其作，它会选择向右移动，直到到达走廊，此时它会选择向上移动，直到到达客厅。显然，如果允许机器人根据其当前状态动态选择要执行的作，它可以做出更好的选择。

策略是从$$\pi:\mathcal{X} \rightarrow \mathcal{A}$$状态到作的映射。指定策略而不是控制磁带有可能通过根据执行过程中发生的实际状态轨迹调整动作序列来显着提高机器人的性能。

下面的代码定义了相当直观的策略。如果在办公室，请向右移动。如果在餐厅或厨房，请向左移动。如果在走廊或客厅，请向上移动。请注意，我们在 python 中将策略实现为一个简单的列表，在索引为 0 的状态（Living Room）中执行的作也是如此 `pi[0]` 。我们将索引返回到中 `action_space` ，以使它后面的代码高效：

```python
RIGHT = vacuum.action_space.index("R")
LEFT  = vacuum.action_space.index("L")
UP    = vacuum.action_space.index("U")
DOWN  = vacuum.action_space.index("D")

reasonable_policy = [UP, LEFT, RIGHT, UP, LEFT]py
```

一旦我们有了给定的策略$$\pi$$，我们就可以采用类似于上述计算控制磁带推出的方式来计算策略推出。特别是，在每种状态下，我们不是从分布$$P(X_{k+1}|a_k,x_k)$$中抽样，而是从分布$$P(X_{k+1}|\pi(x_k),x_k)$$中抽样。换句话说，我们不是模拟预先指定的作$$a_k$$，而是选择$$a_k=\pi(x_k)$$。

下面是一个函数，它计算给定策略的转出，而不是控制磁带：

```python
def policy_rollout(x1, pi, horizon=N):
    """Roll out states given a policy pi, for given horizon."""
    rollout = gtsam.DiscreteValues()
    x = x1
    for k in range(1, horizon):
        a = pi[x]
        rollout[X[k][0]] = x
        rollout[A[k][0]] = a
        next_state_distribution = gtsam.DiscreteDistribution(X[k+1], T[x, a])
        x = next_state_distribution.sample()
    rollout[X[horizon][0]] = x
    return rollout

pretty(policy_rollout(vacuum.rooms.index("Office"), reasonable_policy, horizon=5))
```

| Variable   |   Value     |
| :--------: | :---------: |
|     A1     |      R      |
|     A2     |      U      |
|     A3     |      U      |
|     A4     |      U      |
|     X1     |    Office   |
|     X2     |   Hallway   |
|     X3     | Living Room |
|     X4     | Living Room |
|     X5     | Living Room |

我们基于策略的推出似乎要好得多！

综上所述，我们提出了两种方法来平面作序列，一种是使执行单个作的预期奖励最大化的贪婪方法，另一种是选择固定作序列以最大化作序列的预期效用的基于优化的方法。我们介绍了如何使用控制磁带转出实现后一种方法所需的计算。

随着政策的引入，规划被简化为寻找适当的政策。在最佳情况下，此策略将使预期效用最大化。因为策略是依赖于状态的，所以枚举所有可能策略的组合学排除了任何试图显式枚举候选策略的方法。相反，我们将重点介绍探索与策略相关的效用的方法。正如我们现在将看到的，这比探索固定作序列的效用要复杂一些。

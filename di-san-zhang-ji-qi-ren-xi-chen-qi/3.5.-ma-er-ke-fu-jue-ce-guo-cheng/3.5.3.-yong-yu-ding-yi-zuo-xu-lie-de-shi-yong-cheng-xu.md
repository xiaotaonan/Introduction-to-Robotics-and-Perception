# 3.5.3. 用于定义作序列的实用程序

> 效用是有限或无限地平线上的总折扣奖励。

上面的 greedystrategy 侧重于 applyingan action 的直接好处。对于我们的机器人以及在现实世界中运行的大多数机器人来说，重要的是在较长时间内有效执行，而不仅仅是在下一刻。为了实现长期利益的最大化，我们可以考虑未来获得的所有奖励的总和，而不是只考虑下一阶段的奖励（就像贪婪策略那样）。直接实施此方法有两个直接的缺点：

* 首先，由于动作的效果是不确定的，因此我们对未来的展望越远，我们对机器人预期状态的不确定性就越大。因此，对可能在未来很久以后发生的奖励的考虑打折扣是有意义的，比如有时$$X_{k+1}$$  ，因为 的值会增加$$l$$  。
* 其次，用无限的时间范围进行推理通常很方便，例如，考虑机器人将永远运行的情况。虽然这肯定不是一个现实的期望，但对无限时间范围进行推理通常会简化未来规划的数学复杂性。如果我们只计算所有未来奖励的总和，那么当接近无穷大时$$l$$  ，这个总和将发散到无穷大。

我们可以通过将时间上的奖励步长$$k+l$$乘以折扣因子$$\gamma^l$$来处理这两个缺点，其中$$0 < \gamma \leq 1$$。我们称$$\gamma$$之为折扣系数，将该术语$$\gamma^l R(x_{k+l},a_{k+l},x_{k+l+1})$$\
称为折扣奖励。请注意，对于$$\gamma =1$$，没有折扣，所有未来的奖励都以同等的权重对待。

现在我们使用它来定义一个更通用的 utility 函数。假设机器人执行一系列作$$a_1,\dots,a_n$$，从 state $$X_1=x_1$$开始，并通过 state sequence $$x_1,x_2,x_3 \cdots x_{n+1}$$传递。我们将效用函数$$U:\mathcal{A} \times \mathcal{X}^{n+1} \rightarrow R$$定义为

$$
\begin{equation}
U(a_1, \dots, a_n, x_1, \dots x_{n+1}) =
R(x_1,a_1, x_2) + \gamma R(x_2, a_2, x_3) + \dots \gamma^{n-1} R(x_{n}, a_{n}, x_{n+1})
\end{equation}
$$

我们可以更简洁地将其写成总和

$$
\begin{equation}
U(a_1, \dots, a_n, x_1, \dots x_{n+1}) =
\sum_{k=1}^{n} = \gamma^{k-1} R(x_k, a_k, x_{k+1}) 
\end{equation}
$$

对于$$n < \infty$$，我们将其称为 finite-horizonutility 函数。请注意，未来奖励的影响随时间范围呈指数级减小，并且使用 of $$\gamma^{k-1}$$可确保和在无限范围的情况下收敛（在关于$$R$$的温和假设下）。

上面的表达式是针对特定的 action 序列和 aspecific state 序列定义的。正如我们上面提到的，在规划时，我们无法确定地知道未来的状态。同样，我们可以通过计算给定动作序列的预期效用来解决这个困难。\
$$E[U(a_1,\dots,a_n,X_1,\dots X_n)]$$我们现在可以制定一个稍微复杂一点的规划问题版本：选择最大化 expectedutility 的动作$$a_{1:n}$$序列：

$$
\begin{equation}
a_1^*, \dots a_n^* = \arg  \max_{a_1 \dots a_n \in {\cal A}^n} E[U(a_1, \dots, a_n, X_1, \dots X_{n+1})]
\end{equation}
$$

如上所述，这个问题可以通过简单地列举每个可能的动作序列，并选择使期望最大化的序列来解决。显然，这不是一种计算上易于处理的方法。不仅可能的动作序列的数量随着时间范围呈指数增长$$n$$，而且计算特定动作序列的期望也需要大量的计算。但是，正如我们现在将看到的那样，我们可以使用 rollout 的概念来近似此优化过程。

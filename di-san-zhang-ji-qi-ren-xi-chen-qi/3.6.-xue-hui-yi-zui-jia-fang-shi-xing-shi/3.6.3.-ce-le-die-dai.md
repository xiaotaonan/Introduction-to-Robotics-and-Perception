# 3.6.3. 策略迭代

> 通过迭代改进最优策略的估计，我们最终发现$$\pi^*$$> 。

我们将介绍两种确定最佳策略的方法。我们下面描述的方法，策略迭代，迭代改进候选策略，最终收敛到最优策略$$\pi^*$$。第二种方法，值迭代，迭代改进估计$$V^*$$，最终收敛到最优值函数。但是，两者都需要访问 MDP 的 transitionprobabilities 和 reward function。

策略迭代从对最佳策略的初步猜测开始，然后迭代改进该猜测，直到无法进一步改进。特别是， policy iteration 会生成一系列策略 $$\pi^0,\pi^1,\dots \pi^n$$，这样$$\pi^{i+1}$$is better than policy $$\pi^i$$。当无法进一步改进时，此过程结束，此时$$\pi^{i+1}=\pi^i$$会发生

为了改进策略$$\pi^i$$，我们通过使用$$\pi^i$$代替 来应用 Bellman 方程来更新为每个状态选择的操作$$\pi^*$$。这可以通过以下算法来实现：

从随机策略$$\pi^0$$and$$i=0$$开始，然后重复直到收敛：

* 计算 value 函数$$V^{\pi^i}$$
* 使用更新规则改进每个状态$$x \in X$$  的策略:$$\begin{equation} \pi^{i+1}（x） \leftarrow\arg \max_a Q（x，a;V^{\pi^i}） \end{equation}$$
* 增加$$i$$

请注意，此算法的附带好处是在每次迭代时连续计算 value 函数的更好近似值。由于可以在每种状态中应用的作数量有限，因此更新策略的方法有限。因此，我们预计这种策略迭代算法将在有限的时间内收敛。

我们已经知道如何执行上面的步骤 （1），使用 `calculate_value_function` .该算法的第二步可通过以下代码轻松实现：

```python
def update_policy(value_function):
    """Update policy given a value function"""
    new_policy = [None for _ in range(5)]
    for x, room in enumerate(vacuum.rooms):
        Q_values = [Q_value(value_function, x, a) for a in range(4)]
        new_policy[x] = np.argmax(Q_values)
    return new_policy
```

然后，整个策略迭代算法简单地迭代这些，直到策略不再更改。如果没有给出初始策略，我们可以从 all $$x$$的零值函数$$V^{\pi^0}(x)=0$$\
开始：

```python
def policy_iteration(pi=None, max_iterations=100):
    """Do policy iteration, starting from policy `pi`."""
    for _ in range(max_iterations):
        value_for_pi = vacuum.calculate_value_function(R, T, pi) if pi is not None else np.zeros((5,))
        new_policy = update_policy(value_for_pi)
        if new_policy == pi:
            return pi, value_for_pi
        pi = new_policy
    raise RuntimeError("No stable policy found after {max_iterations} iterations")
```

另一方面，如果我们对初始策略有猜测，我们可以 π0 相应地进行初始化。例如，我们可以从一个不太智能 `always_right` 的策略开始：

```python
RIGHT = vacuum.action_space.index("R")

always_right = [RIGHT, RIGHT, RIGHT, RIGHT, RIGHT]
```

```python
optimal_policy, optimal_value_function = policy_iteration(always_right)
print([vacuum.action_space[a] for a in optimal_policy])
```

```
['L', 'L', 'R', 'U', 'U']
```

从 `always_right` policy 开始，我们的 policy iteration 算法收敛到一个直观上令人愉悦的 policy。我们去 `left` 餐厅和厨房，我们去办公室，我们去 `right` `up` 走廊和餐厅。这与 `always_right` policy （可能更适合命名 `almost_always_wrong` ） 明显不同。事实上，这正是我们在 Section 3.5 中创建的 `reasonable_policy` 。我们已经知道它应该非常擅长尽快到达客厅。事实上，这是最佳的！

我们还在下面打印出最优值函数，说明如果我们靠近客厅，值函数很高，但在餐厅的办公室里则低一点：

```python
for i,room in enumerate(vacuum.rooms):
    print(f"  {room:12}: {optimal_value_function[i]:.2f}")
```

```
  Living Room : 100.00
  Kitchen     : 97.56
  Office      : 85.66
  Hallway     : 97.56
  Dining Room : 85.66
```

当我们在没有策略的情况下开始时，从零值函数开始，也会获得最佳策略：

```python
optimal_policy, _ = policy_iteration()
print([vacuum.action_space[a] for a in optimal_policy])
```

```
['L', 'L', 'R', 'U', 'U']
```

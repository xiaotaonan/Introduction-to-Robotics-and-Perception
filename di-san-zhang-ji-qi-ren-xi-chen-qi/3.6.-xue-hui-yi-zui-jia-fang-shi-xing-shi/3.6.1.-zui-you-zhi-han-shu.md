# 3.6.1. 最优值函数

> 最优策略使价值函数最大化

现在，我们将注意力转向定义最优值函数，该函数可用于构建最优策略$$\pi^{*}$$。从 Section 3.5 中，我们知道如何计算任意策略$$\pi$$的 value 函数：

$$
\begin{equation}
V^\pi(x) = \bar{R}(x,\pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x)) V^\pi(x').
\end{equation}
$$

首先，我们回顾一下 Bellman 在 1960 年的 IEEETransactions on Automatic Control \[ Bellman 和 Kalaba， 1960] 文章中提出的著名最优性原则：

> _最优策略具有这样一种特性，即无论初始状态和初始决策是什么，其余决策都必须构成相对于第一个决策产生的状态的最优策略。_

此原则使推导出最佳策略的递归公式的关键步骤成为可能。事实上，最优价值函数$$\mathcal{V}^*: \mathcal{X}\rightarrow \mathcal{A}$$只是最优政策的价值函数。这在数学上可以写成

$$
\begin{equation}
\begin{aligned}
V^*(x) &= \max_\pi V^{\pi}(x) \\
&=
\max_\pi \left\{ \bar{R}(x,\pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x)) V^\pi(x')   \right\}\\
&=
\max_\pi \left\{ \bar{R}(x,\pi(x)) + \gamma \sum_{x'} P(x'|x, \pi(x)) V^*(x')   \right\}\\
&=
\max_a  \left\{ \bar{R}(x,a) + \gamma \sum_{x'} P(x'|x, a) V^*(x')   \right\} \\
\end{aligned}
\end{equation}
$$

在上面，第二行紧随其后，使用了$$\mathcal{X}^{\pi}$$上面的定义。第三行更有趣。通过应用最优性原理，我们用$$\mathcal{V}^*(x^{\prime})$$替换$$\mathcal{V}^{\pi}(x^{\prime})$$。简单地说，如果来自国家的$$x^{\prime}$$剩余决策必须构成最优政策，则相应的值函数 at $$x^{\prime}$$将是 的最优值函数。$$x^{\prime}$$对于第四行，由于 value 函数已以递归形式编写，$$\pi$$因此仅应用于当前状态（即，在优化中计算时$$\pi$$，它始终显示为$$\pi(x)$$）。因此，我们可以将优化写为针对当前状态下应用的动作的最大化，而不是针对整个策略$$\pi$$的最大化！

这个方程称为贝尔曼方程。它以发现它的数学家理查德·贝尔曼 （Richard Bellman） 的名字命名，它是所有计算机科学中最重要的方程式之一。贝尔曼方程有一个非常好的解释：一个状态的最优值函数是最大预期奖励加上在未来以最佳方式行动时的折扣期望值函数。

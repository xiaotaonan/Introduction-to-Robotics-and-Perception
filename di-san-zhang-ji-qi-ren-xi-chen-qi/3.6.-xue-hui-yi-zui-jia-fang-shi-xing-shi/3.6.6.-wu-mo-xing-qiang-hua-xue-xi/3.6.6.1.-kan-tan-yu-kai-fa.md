# 3.6.6.1. 勘探与开发

以上假设我们通过随机行动来收集数据，但这可能非常低效。事实上，我们可能花费了大量时间 - 字面意思 - 将头撞到墙上。更好的主意可能是一开始随机行动 （探索），但随着时间的推移，花越来越多的时间通过尝试以最佳方式行动 （利用） 来完善最佳策略。

贪婪的行动选择会导致不良的学习结果。我们将以 Q-learning 为例，但其他强化学习方法也存在类似的问题。在 Q-learning 过程中，当达到某个状态$$x$$时，贪婪动作的选择方法是简单地根据当前估计的 Q 值来挑选动作$$a^*$$：

$$
\begin{equation}
a^* = \arg \max_a \hat{Q}(x,a).
\end{equation}
$$

不幸的是，这往往会导致 Q-learning 卡在策略搜索空间的局部最小值中：可能更有希望的状态-动作对永远不会被访问，因为它们的正确（更高）Q 值没有被正确估计，所以它们总是被忽略。

Epsilon-greedy 或$$\epsilon$$-greedy 方法在学习时平衡探索与剥削。与其总是根据当前的估计选择最好的行动，不如简单地在很短的时间内随机选择一个行动，比如说 概率$$\epsilon$$.这就是 epsilon-greedy 方法。的典型值为$$\epsilon$$0.01 甚至 0.1，即我们选择随机行动的 10% 时间。随着时间的推移，也存在减少$$\epsilon$$的计划。

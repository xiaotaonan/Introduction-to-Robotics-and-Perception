# 3.6.5. 基于模型的强化学习

> 只需探索，然后解决 MDP。

我们可以尝试学习 MDP，然后解决它。policy 和 value 迭代都需要访问 transitionprobabilities 和 reward 函数。然而，当面对新环境时，我们可能不知道我们的机器人将如何表现。同样，我们可能无法访问奖励功能：我们如何提前知道在哪里可以找到金罐？

学习 MDP 的一种方法是随机探索。让我们调整上一节中的 `policy_rollout` 代码，以生成 $$(x,a,x^{\prime},r)$$

```python
def explore_randomly(x1, horizon=N):
    """Roll out states given a random policy, for given horizon."""
    data = []
    x = x1
    for _ in range(1, horizon):
        a = np.random.choice(4)
        next_state_distribution = gtsam.DiscreteDistribution(X[1], T[x, a])
        x_prime = next_state_distribution.sample()
        data.append((x, a, x_prime, R[x, a, x_prime]))
        x = x_prime
    return data
```

让我们使用它来创建 499 个体验并显示前 5 个体验：

```python
data = explore_randomly(vacuum.rooms.index("Living Room"), horizon=500)
print(data[:5])
```

```python
[(0, 0, 0, 10.0), (0, 1, 0, 10.0), (0, 3, 3, 0.0), (3, 1, 4, 0.0), (4, 3, 4, 0.0)]
```

我们可以从数据中估计 transitionprobabilities 和 reward 表，然后我们可以使用之前的算法来计算价值函数和/或最优策略。

数学只是我们在上一章的学习部分看到的一个变体。奖励是最容易估计的：

$$
\begin{equation}
R(x,a,x') \approx \frac{1}{N(x,a,x')} \sum_{x,a,x'} r
\end{equation}
$$

其中$$N(x,a,x^{\prime})$$计算记录体验$$(x,a,x^{\prime})$$的次数。转换概率有点棘手：

$$
\begin{equation}
P(x'|x,a) \approx \frac{N(x,a,x)}{N(x,a)}
\end{equation}
$$

其中$$N(x,a)=\sum_{x^{\prime}}N(x,a,x^{\prime})$$是我们在状态中执行作$$a$$的次数$$x$$。

与此相关的代码相当简单，对一些 numpy 技巧进行模运算来处理除以零并广播除法：

```python
R_sum = np.zeros((5, 4, 5), float)
T_count = np.zeros((5, 4, 5), float)
count = np.zeros((5, 4), int)
for x, a, x_prime, r in data:
    R_sum[x, a, x_prime] += r
    T_count[x, a, x_prime] += 1
R_estimate = np.divide(R_sum, T_count, where=T_count!=0)
xa_count = np.sum(T_count, axis=2)
T_estimate = T_count/np.expand_dims(xa_count, axis=-1)
```

上面 `T_count` 对应于 $$N(x,a,x^{\prime})$$ ，变量 `xa_count` 是 $$N(x,a)$$ 。最好检查后者，看看我们的经历是否或多或少具有代表性，即访问了所有状态-动作对：

```python
xa_count
```

```python
array([[24., 22., 18., 22.],
       [17., 34., 21., 28.],
       [24., 25., 20., 23.],
       [22., 26., 22., 17.],
       [29., 43., 27., 35.]])
```

这似乎还不错。如果没有，我们始终可以收集更多数据，我们鼓励您进行试验。

我们可以将真实转换概率$$T$$与估计的转换概率进行比较$$\hat{T}$$，例如，对于客厅：

```python
print(f"ground truth:\n{T[0]}")
print(f"estimate:\n{np.round(T_estimate[0],2)}")
```

```python
ground truth:
[[1.  0.  0.  0.  0. ]
 [0.2 0.8 0.  0.  0. ]
 [1.  0.  0.  0.  0. ]
 [0.2 0.  0.  0.8 0. ]]
estimate:
[[1.   0.   0.   0.   0.  ]
 [0.32 0.68 0.   0.   0.  ]
 [1.   0.   0.   0.   0.  ]
 [0.41 0.   0.   0.59 0.  ]]
```

不错。奖励：

```python
print(f"ground truth:\n{R[0]}")
print(f"estimate:\n{np.round(R_estimate[0],2)}")
```

```python
ground truth:
[[10.  0.  0.  0.  0.]
 [10.  0.  0.  0.  0.]
 [10.  0.  0.  0.  0.]
 [10.  0.  0.  0.  0.]]
estimate:
[[10.   0.  77.1 87.8 77.1]
 [10.   0.  77.1 87.8 77.1]
 [10.  87.8 77.1 87.8 77.1]
 [10.  87.8 77.1  0.  77.1]]
```

总之，在这种情况下的学习可以简单地通过收集大量经验并估计世界行为的模型来完成。之后，您可以使用 policy 或 value iteration 来恢复最佳策略。

# 3.7.2. 推理

贝叶斯网络非常适合建模，在 Section 3.4 中，我们介绍了隐马尔可夫模型 （HMM），它允许我们推理一系列隐藏状态，通过噪声测量观察到。HMM 已经存在了很长时间，并改变了语音识别等领域。它们也正是我们随着时间的推移进行机器人定位所需要的。除了简单的扫地机器人示例之外，它们还可以推广到我们可以使用随时间转换的离散状态建模的几乎任何机器人/环境组合。在我们的示例中，我们只使用单个离散传感器，但 HMM 能够容纳多个传感器，甚至是连续传感器。

然后，我们讨论了使用因子图进行高效推理。朴素的推理，无论是最大后验 （MAP） 还是计算隐藏状态的完整后验，其复杂性都是指数级的。因此，我们引入了一种新的形式主义，因子图，它只关注隐藏变量，捕捉先验、转换和测量 - 假设给定 - 作为因子的影响。我们展示了 HMM 可以很容易地转换为因子图，但这实际上适用于具有给定动作和测量值的任何动态贝叶斯网络。然后，我们草拟了 - 对于 HMM - 最大乘积和和积算法，它们分别计算 MAP 估计和完整后验，时间复杂度在时间步长数量上是线性的。这就是动态规划的力量。

在 3.5 节中，我们介绍了马尔可夫决策过程或 MDP，我们用它来在随机环境中对决策进行建模，尽管对状态有完全的了解。这是一个丰富的主题，我们引入了许多新概念，包括奖励、效用函数、推出和策略及其价值。

最后，我们在 Section 3.6 中考虑了学习最优策略。在推导出 Bellman 方程之后，我们讨论了两种计算最佳策略/价值函数的算法：策略迭代和价值迭代。然而，这两者都假设你完全了解世界模型（状态之间的转换概率）和奖励结构。在基于模型的方法中，我们尝试根据通过探索环境收集的经验来估计这些。但是，一种更直接的专注于估计作值（Q 值）的方法是 Q-learning，这是无模型方法的一个示例。最后，我们讨论了如何平衡开发和探索，以避免陷入政策空间的局部角落。

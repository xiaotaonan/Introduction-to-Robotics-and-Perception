# 3.5. 马尔可夫决策过程

> 对于受控马尔可夫链，规划是选择控制输入的过程。这导致了马尔可夫决策过程 （MDP） 的概念。

之前，在 Section 3.2 中，我们描述了如何使用条件概率分布来模拟动作效果的不确定性。我们将信念状态$$b_{k+1}$$定义为给定动作$$a_1 \dots a_k$$顺序的时间状态的后验概率$$P(X_{k+1}|a_1 \cdots a_k)X_{k+1}$$。\
$$K+1$$在每个示例中，动作的顺序都是预先确定的，我们只计算了与从某个指定的初始状态执行这些动作相关的概率，由概率分布$$P(X_1)$$控制。

在本节中，我们将考虑选择要执行的作的问题。做出这些决定需要我们有量化的标准来评估行动及其效果。我们使用 reward 函数对这些标准进行编码。由于作的效果是不确定的，因此无法知道通过执行特定作（或一系列作）将获得的奖励。因此，我们将再次调用期望的概念来计算应用作的预期未来收益。

当我们研究这些概念时，很快就会发现，执行预定义的作序列并不是面对未来的最佳方式。例如，假设我们的扫地机器人想从办公室搬到客厅。因为不可能知道我们应该 “向右移动” 多少次才能到达走廊，所以我们可能会构建一个动作序列，向右移动很多次（以增加到达餐厅的概率），然后向上移动很多次（增加到达厨房的概率），然后向左移动很多次（以增加到达客厅的概率）。

如果机器人无法知道其当前位置，这种计划可能有意义。例如，长时间向右移动最终应该很有可能将机器人带到餐厅，但如果机器人向右移动一小段时间，它可能会留在办公室、到达走廊或到达餐厅。

相反，如果机器人能够知道自己的位置（利用感知），那么当它到达走廊时，它可以投机取巧地采取行动，并立即向上移动。这里的关键思想是，任何时刻的最佳作都取决于执行作的状态。在每种状态中执行哪个作的配方称为 apolicy，定义策略及其关联的值函数是本节的主要目标。

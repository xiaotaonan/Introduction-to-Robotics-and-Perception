# 2.2.5 概率论与统计学

> 概率论研究的是某些数学函数，而统计学研究的是数据的函数。两者既相关又有区别。

概率论和统计学似乎关注的是同一种思想，但它们是两个截然不同的研究领域。

概率论是研究一类数学函数（概率分布）的学科。现代概率论的公理化方法始于三个公理，所有其他性质都由此推导出来：

* 对于$$A \subseteq \Omega$$  ,$$P(A) \geq 0$$
* $$P(\Omega)=1$$
* 对于$$A_i,A_j \subseteq \Omega$$  ，如果$$A_i \cap A_j = \theta$$  ，则$$P(A_i \cup A_j)=P(A_i)+P(A_j)$$  。

概率论并不考虑如何获得概率分布$$P$$的问题。概率论者将其与公理一起视为既定事实。

期望是概率分布的一个属性。对于离散随机变量$$X$$，其$$\Omega=\{x_1...x_n\}$$，$$E[X]$$（也称为平均值，通常表示为$$\mu$$）可以按上述方法计算:

* [ ] $$\mu=E[X]=\sum_{i=1}^{n}x_ip_X(x_i)$$ \[公式 2.8]

随机变量的方差通常用$$\sigma^2$$表示，它仅仅是随机变量$$X$$与均值的平方差的期望值。方差也是概率分布的一个属性，可以通过以下公式计算：

$$\sigma^2=E[(X-\mu)^2]=\sum_{i=1}^{n}p_X(x_i)(x_i-\mu)^2$$ (公式 2.9)

请注意，$$\mu$$和$$\sigma^2$$的表达式仅取决于概率分布（在本例中为 PMF$$p_X$$）和随机变量$$X$$所取的值。

统计量是数据的任何函数（包括恒等函数）。考虑一组测量值$$\{z_1,...z_n\}$$\
。这些值的平均值（通常表示为$$\overline z$$）是一个统计量，它可以计算为：

$$\overline z=\frac{1}{N}\sum_{i=1}^{N}z_i$$ (公式2.10)

同样，数据的方差（通常用$$\sigma^2$$表示）可以计算为：

$$\hat \sigma^2 = \frac{1}{N-1}\sum_{i=1}^N(z_i-\overline z)^2$$ (公式2.11)

请注意，$$\overline z$$和$$\hat z^2$$的定义仅取决于数据本身。

这两组定义之间有一些显而易见的相似之处。随机变量的均值似乎与数据集的平均值相似。随机变量的方差似乎与数据集的方差相似。

事实上，如果某些概率分布恰好能够很好地描述世界的行为方式，那么概率论就可以为数据推理系统提供严格的基础。例如，如果我们观察到的数据符合概率分布$$p_X$$，那么数据的平均值将趋向于预期值$$X$$。这个性质可以正式写成弱大数定律，即对于任意的$$\epsilon >0$$，如果$$\overline z N$$表示大小为$$N$$的数据集的平均值，那么：

$$\lim \limits_{N \rightarrow \infty } P(|\overline  z N-\mu| < \epsilon) =1$$(公式 2.12)

也就是说，随着$$N$$的增大，$$N$$个数据点的平均值将任意接近$$\mu$$。这种情况发生的概率为 $$1$$，我们在此不讨论这个细微差别。这解释了为什么通过采样进行模拟有效，以及为什么结果会随着样本数量的增加而趋于改善。

弱大数定律只是众多形式化概率论与统计学之间联系的定理中的第一个，但这些定理大多表达了一个简单的概念：随着数据集规模的扩大，该数据集的统计数据将越来越接近于生成该数据集的底层概率分布的各种属性。因此，利用概率论来推理分布可以作为对现实世界进行推理的基础。

# 第三章 机器人吸尘器

> 我们将随着时间的推移推断离散状态空间中的概率动作。

我们将要讨论的第二个机器人是一个移动机器人，其原型是我们许多人已经熟悉的真空吸尘器机器人。我们假设该机器人配备了硬件和软件，可以执行导航、运动规划和运动控制，但我们不会具体说明这些功能是如何实现的。这一假设使我们能够专注于高级问题（例如，决定下一步要清洁哪个房间），而无需担心低级细节（例如，规划覆盖特定房间的具体路径，或导航穿过门口）。

除了我们的真空吸尘机器人可&#x4EE5;_&#x79FB;&#x52A8;_&#x8FD9;一显而易见的事实之外，我们的真空吸尘机器人与上一章讨论的垃圾分类机器人还有几个关键的区别。首先，动作的效果取决于当前的世界状态；如果一个机器人在客厅里并向左移动，它到达的位置将与它在办公室开始时的位置不同。其次，真空吸尘机器人执行的动作具有不确定的效果。这与垃圾分类机器人的动作截然不同，垃圾分类机器人的动作是确定性地实现其目标，而不管当前状态如何（“将物体移动到金属垃圾桶”动作将物体移动到金属垃圾桶，而不管物体的类别如何，并且可靠性为 100％）。第三，由于动作的效果取决于状态，因此未来目标的实现将取决于机器人现在执行的动作（因为当前动作会影响未来状态）。因此，这个机器人必须考虑世界状态如何随着时间的推移而演变。

在本章中，我们将探讨行动的概率结果。

对于我们的真空清洁机器人，状态对应于房屋中的房间，轨迹代表机器人从一个房间移动到另一个房间。

我们将使用条件概率分布来建模不确定的动作，类似于上一章中对传感器测量的建模方式。利用这些分布，我们可以将特定动作序列的不确定性随时间向前传播，并从相应的概率分布生成样本轨迹。

本章中我们对感知的讨论将非常有限。

我们将模拟一个简单的离散光传感器。然而，由于传感器测量依赖于状态，而状态又依赖于执行的操作序列，因&#x6B64;_&#x611F;&#x77E5;_&#x6210;为一个更为复杂的问题。

虽然我们的垃圾分类机器人依靠仅使用当前传感器读数的简单 MLE 或 MAP 估计，但真空清洁机器人需要将其动作历史（具有不确定的影响）的知识与传感器测量序列相结合。

我们将使用**隐马尔可夫模型 (HMM)** 来解决这一感知问题，该模型定义了随时间变化的感知概率模型。至关重要的是，我们将演示如何将这些模型转换为**因子图** ，从而能够在给定一系列传感器测量值和动作的情况下高效计算出最可能的状态序列。

我们的真空吸尘机器人的规划变得更加复杂。

不是选择单一行动来最小化当前步骤的成本， 我们将推断一段时间内一系列的动作。 为了实现这一目标，我们将引入**马尔可夫决策过程 (MDP)** 。MDP 引入&#x4E86;_&#x5956;&#x52B1;_&#x7684;概念，使我们能够识别最优行动。我们还将推导出最&#x4F18;_&#x7B56;略_ ——一种指定在每个状态下采取什么行动以最大化随时间推移的总奖励的策略。

最后，我们将介绍强化学习的概念，其中使用机器人正常运行期间收集的数据来估计 MDP 的参数。
